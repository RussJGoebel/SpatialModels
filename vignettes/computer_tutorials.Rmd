---
title: "Computer Tutorials"
output: 
  rmarkdown::html_vignette:
    code_folding: hide
vignette: >
  %\VignetteIndexEntry{Computer Tutorials}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this document, we follow tutorials from the textbook *Hierarchical Modeling and Analysis for Spatial Data* in order to explore and demonstrate spatial modeling implementations. Much of the text written in this file is direct quotes from this textbook.

## Load Packages

```{r setup}
library(SpatialModels)

#Packages for Computer Tutorials
library(maps)
library(maptools)
library(spdep)
library(spData) # used to find datasets originally in the spdep package
library(spatialreg)
library(classInt) # used for plotting maps
library(RColorBrewer) # used for colored maps

```

*Note: The package `maptools` was retired in 2023. We installed it using a snapshot with `install.packages("maptools", repos = "https://packagemanager.posit.co/cran/2023-10-13")`.*

*Note: The packages `sf` and `terra` seem to be required for maptools.*

*`maps`: Draw Geographical Maps*

*`maptools`: Set of tools for manipulating geographic data*

*`spdep`: Spatial Dependence: Weighting Schemes, Statistics; A collection of functions to create spatial weights matrix objects...*

*`classInt`: Selected commonly used methods for choosing univariate class intervals for mapping or other graphics purposes.*

*`RColorBrewer`: Provides color schemes for maps (and other graphics) designed by Cynthia Brewer as described at http://colorbrewer2.org*


## 4.5.1: Adjacency Matrices from maps using spdep

The idea here seems to be that adjacency matrices can be computed for US states and counties using the `maps` and `maptools` packages. "The idea is to create a sequence of data structures that will eventually produce an adjacency matrix."

For example, an adjacency matrix for the US states can be created:

```{r class.source = 'fold-show'}
usa.state <- map(database = "state", fill = TRUE, plot = FALSE)
state.ID <- sapply(strsplit(usa.state$names, ":"), function(x) x[1])
usa.poly <- map2SpatialPolygons(usa.state, IDs = state.ID)
usa.nb <- poly2nb(usa.poly)
usa.adj.mat <- nb2mat(usa.nb, style = "B")
```

*Notes on output:

`usa.state` is a "map" class object returned by the `map()` function. It has fields `x`, `y`, `range` and `names`. As per the documentation for `map()`, this object can be used as a database for successive calls to `map` and functions. Here, since `fill = TRUE`, the `x` and `y` vectors have coordinates of successive polygons, so the return value can be handed directly to `lines` or `polygon` as appropriate.


```{r}
head(usa.state$x)
head(usa.state$y)
head(usa.state$range)
head(usa.state$names)
```

`state_ID` extracts the state from the names given by `map`. 

```{r}
head(state.ID)
```
`usa.poly` is the output of the `map2SpatialPolygons` function, which convert map objects from the `maps` package into suitable objects defined in the `sp` package. This is a `SpatialPolygons` object.

`usa.nb` is the output of the `poly2nb` function from the `spdep` package. This is a neihgbors list with class `nb`. 

`usa.adj.mat` is an adjacency matrix generated by the `nb2mat` function from the `spdep` package. The option `style = "B"` produces the basic binary coding. 

```{r}
head(usa.adj.mat)
```

We can also produce adjacency matrices for counties:

```{r}
mn.county <- map("county","minnesota", fill = TRUE, plot = FALSE)
county.ID <- sapply(strsplit(mn.county$names,","), function(x) x[2])
mn.poly <- map2SpatialPolygons(mn.county, IDs = county.ID)
mn.nb <- poly2nb(mn.poly)
mn.adj.mat <- nb2mat(mn.nb,style = "B")
```

We can find which counties neighbor eachother:

```{r}
mn.region.id <- attr(mn.nb, "region.id")
winona.neighbors.index <- mn.nb[[match("winona",mn.region.id)]]
winona.neighbors <- rownames(mn.adj.mat[winona.neighbors.index,])
winona.neighbors
```

We can extract the counties using the attributes of the `nb` object. The `match` function is a base function which returns a vector of positions of first matches of its first argument with its second.

Note that this only found adjacent counties in Minnesota. There are others in Wisconson.

### Creating Adjacency matrices using shapefiles

There are ways to import shapefiles, though I am not sure where to find the dataset from the textbook.

```{r, eval = FALSE}
# The following won't run without the datasets, hence eval = FALSE:
mn.map.shp <- readShapeSpatial("minnesota.shp")
mn.nb.shp <- poly2nb(mn.map.shp)
mn.adj.mat.shp <- nb2mat(mn.nb, style = "B")
```

# Moran's *I* and Geary's *C* in `spdep`

Moran's *I* and Geary's *C* are measures of spatial association.

First, we use the `nb` object `usa.nb` and convert it to a `listw` object.

```{r}
usa.listw <- nb2listw(usa.nb, style = "W")
```

The `nb2listw` function supplements a neighbours list with spatial weights for the chosen coding scheme. Here, the `style = "W"` option takes the 0-1 neighbors list and creates row-normalized weights.

Next, we read the SAT scores using the dataset available in `http://www.biostat.umn.edu/~brad/data2.html`. *Note: this webpage links to the supplemental materials page for the textbook, and the corresponding dataset is not quite the same, as there are no headings. Thus, we include our own headings.

In order to maintain the organization of this package, we have an extra step to extract the file path. For general analysis, we can replace `file_path` with the filepath needed.

It seems like the 

##Columns: 1. state name 
##         2. verbal score on SAT
##         3. math score on SAT
##         4. % of eligible students taking exam in that state

```{r}
# This line is only needed for us to do this analysis within an R package.
file_path <- system.file("HierarchicalModelingAndAnalysis_SupplementalFiles","state-sat.dat",package = "SpatialModels")

# file_path can be replaced with an appropriate file path for more general analysis.
state.sat.scores <- read.table(file_path, col.names = c("STATE","VERBAL","MATH","ELIGIBLE"))

# To align with our other dataset, we have to only include the contiguous data from the US: we remove Alaska, Hawaii, and the overall US average.

non_contiguous <- c("alaska","hawaii","us")
non_contiguous_indices <- match(non_contiguous,state.sat.scores$STATE)
state.sat.scores <- state.sat.scores[c(-non_contiguous_indices),]
```

Next, we use the `moran.test` function to obtain

```{r}
moran.test(state.sat.scores$VERBAL,listw = usa.listw)
```
Likewise, Geary's *C* can be computed analogously using the `geary.test` function:

```{r}
geary.test(state.sat.scores$VERBAL, listw = usa.listw)
```

For maps with "islands" or regions without neighbors, we need to set `zero.policy = TRUE` in the `nb2listw()` function.

# `SAR` and `CAR` model fitting using `spdep` in `R`.

In this example, we analyze data for SIDS (sudden infant death syndrome).

First, we load the data, which is in the `sids` package:

```{r}
nc.sids <- readShapePoly(system.file("shapes/sids.shp",package = "spData")[1],
                         ID = "FIPSNO", proj4string = CRS("+proj= longlat +ellps=clrk66"))


rn <- sapply(slot(nc.sids,"polygons"),function(x) slot(x,"ID"))
ncCC89.nb <- read.gal(system.file("weights/ncCC89.gal",package = "spData")[1],region.id=rn)
```
The first step produces a `SpatialPolygonsDataFrame` object, while the second produces the region IDs and stores them in `rn`. The third uses the IDs to produce a `nb` object by directly reading from the GAL file.

Next, we use a Freeman-Tukey transformation to produce the transformed rates and append them to the `nc.sids` object.

```{r}
nc.sids.rates.FT <- sqrt(1000)*(sqrt(nc.sids$SID79/nc.sids$BIR79)+sqrt((nc.sids$SID79+1)/nc.sids$BIR79))

nc.sids$rates.FT <- nc.sids.rates.FT
```

We wish to regress these rates on the non-white birth rates over the same period. This variable is available as `NWBIR79` in the `nc.sids` object. We will use the Freeman-Tukey transformed birth rates:

```{r}
nc.sids.nwbir.FT = sqrt(1000)*(sqrt(nc.sids$NWBIR79/nc.sids$BIR79)+sqrt((nc.sids$NWBIR79+1)/nc.sids$BIR79))

nc.sids$nwbir.FT <- nc.sids.nwbir.FT
```

Maximum likelihood estimation of (4.23) can be produced using the `errorsarlm()` or equivalently the `spautolm()` function in `spdep`. These functions produce the same output. Below, we demonstrate the latter.

We first create a `listw` object using the 0-1 adjacency structure

```{r}
ncCC89.listw <-  nb2listw(ncCC89.nb, style = "B", zero.policy = TRUE)
```

Note that the `zero.policy = TRUE` is required here because the county shapefile hast two countries that have zero neighbors.

To identify these countries we can write

```{r}
nc.county.id <-  attr(ncCC89.nb, "region.id")
nc.no.neighbors <- card(ncCC89.nb)
nc.islands <- as.character(nc.sids[card(ncCC89.nb) == 0,]$NAME)
nc.islands
```

Here, `card` tallies the number of neighbours of regions in the neighbours list.

We now estimate the SAR model using

```{r}
nc.sids.sar.out <- spautolm(rates.FT~nwbir.FT,data = nc.sids, family = "SAR", listw = ncCC89.listw, zero.policy = TRUE)
summary(nc.sids.sar.out)
```


We can plot the fitted rates on a map.

```{r}
nc.sids$fitted.sar <- fitted(nc.sids.sar.out)
brks <- c(0,2,3,3.5,6)
color.pallete <- rev(brewer.pal(4,"RdBu"))
class.fitted <- classIntervals(var = nc.sids$fitted.sar, n = 4, style= "fixed", fixedBreaks = brks, dataPrecision = 4)
color.code.fitted <- findColours(class.fitted, color.pallete)
plot(nc.sids, col = color.code.fitted)
```

We can plot the raw rates analagously:

```{r}

class.fitted <- classIntervals(var = nc.sids$rates.FT, n = 4, style= "fixed", fixedBreaks = brks, dataPrecision = 4)
color.code.fitted <- findColours(class.fitted, color.pallete)
plot(nc.sids, col = color.code.fitted)
```

Instead of defining a neighbourhood structure completely in terms of spatial adjacency on the map, we may want to construct neighbors using a distance funciton. For example, given centroids of the various regions, we could identify regions as neighbors if and only if their intercentroidal distance is below a particular threshold.

We illustrate using a dataset offering neighborhood-level information on crime, mean home value, mean income, and other variables for 49 neighborhoods in Columbus, OH, during 1980.We begin by reading the data and creating a `SpatialPolygonsDataFrame` object.

```{r}
library(spdep)
columbus.poly <- readShapePoly(system.file("etc/shapes/columbus.shp", package = "spdep")[1])


```
Suppose we would like to have regions with intercentroidal distances less than half the maximum intercentroidal distance as neighbors. We first construct an object, `columbus.coords`, that contain the centroids of the different regions.

```{r}
columbus.coords <- coordinates(columbus.poly)
```

A particualrly useful function provided by `spdep` is the `dnearneigh()` function. This function can take a matrix of coordinates along with a specified lower and upper distance bounds as inputs and returns a list of neighbors categorized by all points that are within the specified distance threshold of eachother. Note that the function uses Euclidean distances, which means that the coordinates must be projected onto a plane and cannot be in terms of latitude and longitude before the function is applied. There is, howeer, a potential pitfall of directly using the function in that it might create islands unless we are judicious about our choice of upper distance bound. One way to circumvent this problem is to first apply k-nearest-neighbors and then to create a neighborhood list from these k-nearest-neighbors.

```{r}
columbus.knn <- knearneigh(columbus.coords, k = 1) # 1-nearest-neighbors
columbus.knn2nb <- knn2nb(columbus.knn) # 
```

Next, the Euclidian distances between neighbors are constructed by applying the `nbdists()` function to the k-nearest neighbor list. This returns the distances.

```{r}
columbus.dist.list <- nbdists(columbus.knn2nb, columbus.coords)
columbus.dist.vec <- unlist(columbus.dist.list)
```

Next, we find the maximum of the nearest neighbor distances and pass it to the `dnearneigh()` function as an input for the upper bound.

```{r}
columbus.dist.max <- max(columbus.dist.vec)
columbus.dnn.nb <- dnearneigh(columbus.coords, 0, columbus.dist.max)
```

This ensures there are no islands. Next, we create a `listw` object from the returned `dnearneigh()` function and estimate the SAR model.

```{r}
columbus.dnn.listw <- nb2listw(columbus.dnn.nb, style = "B", zero.policy = TRUE)
columbus.dnn.sar.out <- spautolm(CRIME~HOVAL+INC, data = columbus.poly, listw = columbus.dnn.listw, family = "SAR", zero.policy = TRUE)
```


